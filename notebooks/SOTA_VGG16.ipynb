{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, utils\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Image and Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the train, test, and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = utils.image_dataset_from_directory(\"datasets/data/split/vehicles_zenodo/train\", image_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE)\n",
    "val = utils.image_dataset_from_directory(\"datasets/data/split/vehicles_zenodo/val\", image_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE)\n",
    "test = utils.image_dataset_from_directory(\"datasets/data/split/vehicles_zenodo/test\", image_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data for VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.map(lambda x, y: (tf.keras.applications.vgg16.preprocess_input(x), y))\n",
    "val = val.map(lambda x, y: (tf.keras.applications.vgg16.preprocess_input(x), y))\n",
    "test = test.map(lambda x, y: (tf.keras.applications.vgg16.preprocess_input(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.VGG16(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "    pooling='avg',\n",
    "    classes=6,\n",
    "    classifier_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"models/sota/SOTA_VGG16.h5\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "  train.cache(),\n",
    "  epochs=20,\n",
    "  callbacks=callbacks_list,\n",
    "  validation_data=val.cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test)\n",
    "\n",
    "load_model = keras.models.load_model(\"models/sota/SOTA_VGG16.h5\")\n",
    "load_model.evaluate(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
